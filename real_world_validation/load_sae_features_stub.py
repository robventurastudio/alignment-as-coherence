"""Lab-specific hook for loading SAE-derived feature activations."""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Sequence, Union

import numpy as np

_DEFAULT_ARCHIVE_NAME = "sae_activations.npz"
_ENV_PATH_VARIABLE = "REAL_WORLD_VALIDATION_SAE_PATH"


@dataclass
class SAEFeatureBundle:
    """Container for the feature activations needed by the validators."""

    feature_index: Sequence[int]
    pre_training_activations: np.ndarray
    post_training_activations: np.ndarray
    target_alignment_activations: np.ndarray
    metadata: Optional[Dict[str, object]] = None

    def validate_shapes(self) -> None:
        """Ensure the activation arrays are compatible for downstream use."""

        shapes = {
            "pre_training_activations": self.pre_training_activations.shape,
            "post_training_activations": self.post_training_activations.shape,
            "target_alignment_activations": self.target_alignment_activations.shape,
        }
        if len({shape for shape in shapes.values()}) != 1:
            raise ValueError(
                "All activation tensors must share the same shape. "
                f"Observed shapes: {shapes}"
            )

    def to_serializable(self) -> Dict[str, object]:
        """Return a JSON-friendly representation of the bundle."""

        metadata = dict(self.metadata or {})
        return {
            "feature_index": [int(idx) for idx in self.feature_index],
            "pre_training_activations": self.pre_training_activations.tolist(),
            "post_training_activations": self.post_training_activations.tolist(),
            "target_alignment_activations": self.target_alignment_activations.tolist(),
            "metadata": metadata,
        }


def _generate_synthetic_bundle(num_features: int = 64) -> SAEFeatureBundle:
    """Provide a deterministic synthetic dataset for quick smoke tests."""

    x_axis = np.linspace(0.0, 1.0, num_features, dtype=np.float64)
    feature_index = np.arange(num_features, dtype=int)

    pre_training = np.sin(2 * np.pi * x_axis) * 0.25
    post_training = pre_training * 0.8 + 0.05 * np.cos(4 * np.pi * x_axis)
    target_alignment = np.clip(post_training + 0.1 * x_axis, -1.0, 1.0)

    metadata = {
        "source": "synthetic-demo",
        "num_features": int(num_features),
        "description": "Deterministic fallback generated by load_sae_features.",
    }

    bundle = SAEFeatureBundle(
        feature_index=feature_index.tolist(),
        pre_training_activations=pre_training.astype(np.float64),
        post_training_activations=post_training.astype(np.float64),
        target_alignment_activations=target_alignment.astype(np.float64),
        metadata=metadata,
    )
    bundle.validate_shapes()
    return bundle


def _load_bundle_from_npz(path: Path) -> SAEFeatureBundle:
    """Load SAE activations from a NumPy ``.npz`` archive."""

    archive = np.load(path, allow_pickle=True)

    required_keys = {
        "feature_index",
        "pre_training_activations",
        "post_training_activations",
        "target_alignment_activations",
    }
    missing = required_keys - set(archive.files)
    if missing:
        raise KeyError(
            f"Archive {path} is missing required keys: {sorted(missing)}"
        )

    feature_index = archive["feature_index"].astype(int).tolist()
    pre_training = np.asarray(archive["pre_training_activations"], dtype=np.float64)
    post_training = np.asarray(archive["post_training_activations"], dtype=np.float64)
    target_alignment = np.asarray(
        archive["target_alignment_activations"], dtype=np.float64
    )

    metadata: Optional[Dict[str, object]] = None
    if "metadata" in archive.files:
        raw_metadata = archive["metadata"]
        if isinstance(raw_metadata, np.ndarray) and raw_metadata.shape == ():
            metadata_obj = raw_metadata.item()
            if isinstance(metadata_obj, dict):
                metadata = {str(k): metadata_obj[k] for k in metadata_obj}
        elif isinstance(raw_metadata, dict):
            metadata = {str(k): raw_metadata[k] for k in raw_metadata}

    bundle = SAEFeatureBundle(
        feature_index=feature_index,
        pre_training_activations=pre_training,
        post_training_activations=post_training,
        target_alignment_activations=target_alignment,
        metadata=metadata or {},
    )
    bundle.validate_shapes()
    bundle.metadata.setdefault("source", str(path))
    return bundle


def _candidate_paths(project_root: Optional[Union[str, Path]]) -> Sequence[Path]:
    """Return possible archive locations based on user input and env vars."""

    candidates = []

    env_path = os.environ.get(_ENV_PATH_VARIABLE)
    if env_path:
        env_candidate = Path(env_path).expanduser()
        candidates.append(env_candidate)

    if project_root is not None:
        root_path = Path(project_root).expanduser()
        if root_path.is_file():
            candidates.append(root_path)
        else:
            candidates.append(root_path / _DEFAULT_ARCHIVE_NAME)

    return tuple(dict.fromkeys(candidates))  # preserve order while deduplicating


def load_sae_features(
    project_root: Optional[Union[str, Path]] = None,
    *,
    allow_synthetic_fallback: bool = True,
) -> SAEFeatureBundle:
    """Load Sparse Autoencoder feature activations from lab-specific storage.

    Parameters
    ----------
    project_root : Optional[Union[str, Path]]
        Optional path handle or project root. Replace this placeholder with
        the location of your serialized SAE activations (NumPy archives, HF
        datasets, parquet files, etc.). When a directory is supplied the
        loader will look for ``sae_activations.npz`` within it. You can also
        override the path via the ``REAL_WORLD_VALIDATION_SAE_PATH``
        environment variable.
    allow_synthetic_fallback : bool, default ``True``
        If ``True`` (the default) a deterministic synthetic bundle is returned
        when no archive is available. Set to ``False`` to require a concrete
        dataset and raise an informative error instead.

    Returns
    -------
    SAEFeatureBundle
        Structured activations ready to be mapped into coherence fields.

    Notes
    -----
    Replace this helper with code that interfaces with your internal storage
    systems (object stores, experiment trackers, etc.). The helper
    :meth:`SAEFeatureBundle.validate_shapes` is available to perform basic
    sanity checks after loading.
    """

    for candidate in _candidate_paths(project_root):
        if candidate.is_file():
            return _load_bundle_from_npz(candidate)

    if allow_synthetic_fallback:
        return _generate_synthetic_bundle()

    raise FileNotFoundError(
        "No SAE activation archive found. Provide a valid path or enable "
        "the synthetic fallback."
    )


__all__ = ["SAEFeatureBundle", "load_sae_features"]
